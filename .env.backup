/======AZURE_SETTINGS=======/

AZURE_DEPLOYMENT_NAME=gpt-4.1
AZURE_ENDPOINT=https://oai-iuliano.openai.azure.com/
AZURE_API_KEY=22hiwYlZr8vxt6IQ2YlwI0Ty3fesSPUXSZxd8KFNu9SBvUf9G0JOJQQJ99BIACYeBjFXJ3w3AAABACOGW44w


/======LOCAL_SETTINGS=======/

EXAMPLE_LOCAL_CURL='''curl http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen/qwen3-4b-thinking-2507",
    "messages": [
      { "role": "system", "content": "Always answer in rhymes. Today is Thursday" },
      { "role": "user", "content": "What day is it today?" }
    ],
    "temperature": 0.7,
    "max_tokens": -1,
    "stream": false
}'
'''
LOCAL_ENDPOINT=http://localhost:1234/v1/chat/completions
LOCAL_MODEL=qwen/qwen3-4b-thinking-2507



/======OPENROUTER_SETTINGS=======/

OPENROUTER_API_KEY=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
OPEROUTER_FREE_MODELS='''
Qwen: Qwen2.5 VL 32B Instruct (free)

101M tokens
Qwen2.5-VL-32B is a multimodal vision-language model fine-tuned through reinforcement learning for enhanced mathematical reasoning, structured outputs, and visual problem-solving capabilities. It excels at visual analysis tasks, including object recognition, textual interpretation within images, and precise event localization in extended videos. Qwen2.5-VL-32B demonstrates state-of-the-art performance across multimodal benchmarks such as MMMU, MathVista, and VideoMME, while maintaining strong reasoning and clarity in text-based tasks like MMLU, mathematical problem-solving, and code generation.

by qwen
8K context
$0/M input tokens
$0/M output tokens
NVIDIA: Nemotron Nano 9B V2 (free)

76M tokens
NVIDIA-Nemotron-Nano-9B-v2 is a large language model (LLM) trained from scratch by NVIDIA, and designed as a unified model for both reasoning and non-reasoning tasks. It responds to user queries and tasks by first generating a reasoning trace and then concluding with a final response.   The model's reasoning capabilities can be controlled via a system prompt. If the user prefers the model to provide its final answer without intermediate reasoning traces, it can be configured to do so.
'''
by nvidia
128K context
$0/M input tokens
$0/M output tokens
MoonshotAI: Kimi VL A3B Thinking (free)

74,4M tokens
Kimi-VL is a lightweight Mixture-of-Experts vision-language model that activates only 2.8B parameters per step while delivering strong performance on multimodal reasoning and long-context tasks. The Kimi-VL-A3B-Thinking variant, fine-tuned with chain-of-thought and reinforcement learning, excels in math and visual reasoning benchmarks like MathVision, MMMU, and MathVista, rivaling much larger models such as Qwen2.5-VL-7B and Gemma-3-12B. It supports 128K context and high-resolution input via its MoonViT encoder.

by moonshotai
131K context
$0/M input tokens
$0/M output tokens
OpenAI: gpt-oss-120b (free)

49,7M tokens
gpt-oss-120b is an open-weight, 117B-parameter Mixture-of-Experts (MoE) language model from OpenAI designed for high-reasoning, agentic, and general-purpose production use cases. It activates 5.1B parameters per forward pass and is optimized to run on a single H100 GPU with native MXFP4 quantization. The model supports configurable reasoning depth, full chain-of-thought access, and native tool use, including function calling, browsing, and structured output generation.

by openai
33K context
$0/M input tokens
$0/M output tokens
Tencent: Hunyuan A13B Instruct (free)

39,3M tokens
Hunyuan-A13B is a 13B active parameter Mixture-of-Experts (MoE) language model developed by Tencent, with a total parameter count of 80B and support for reasoning via Chain-of-Thought. It offers competitive benchmark performance across mathematics, science, coding, and multi-turn reasoning tasks, while maintaining high inference efficiency via Grouped Query Attention (GQA) and quantization support (FP8, GPTQ, etc.).

by tencent
33K context
$0/M input tokens
$0/M output tokens
Mistral: Mistral Small 3 (free)

34,8M tokens
Mistral Small 3 is a 24B-parameter language model optimized for low-latency performance across common AI tasks. Released under the Apache 2.0 license, it features both pre-trained and instruction-tuned versions designed for efficient local deployment.  The model achieves 81% accuracy on the MMLU benchmark and performs competitively with larger models like Llama 3.3 70B and Qwen 32B, while operating at three times the speed on equivalent hardware. Read the blog post about the model here.

by mistralai
33K context
$0/M input tokens
$0/M output tokens
Google: Gemma 3n 2B (free)

33,1M tokens
Gemma 3n E2B IT is a multimodal, instruction-tuned model developed by Google DeepMind, designed to operate efficiently at an effective parameter size of 2B while leveraging a 6B architecture. Based on the MatFormer architecture, it supports nested submodels and modular composition via the Mix-and-Match framework. Gemma 3n models are optimized for low-resource deployment, offering 32K context length and strong multilingual and reasoning performance across common benchmarks. This variant is trained on a diverse corpus including code, math, web, and multimodal data.

by google
8K context
$0/M input tokens
$0/M output tokens
Google: Gemma 2 9B (free)

29,6M tokens
Gemma 2 9B by Google is an advanced, open-source language model that sets a new standard for efficiency and performance in its size class.  Designed for a wide variety of tasks, it empowers developers and researchers to build innovative applications, while maintaining accessibility, safety, and cost-effectiveness.  See the launch announcement for more details. Usage of Gemma is subject to Google's Gemma Terms of Use.
'''
by google
8K context
$0/M input tokens
$0/M output tokens
Google: Gemma 3 12B (free)

29M tokens
Gemma 3 introduces multimodality, supporting vision-language input and text outputs. It handles context windows up to 128k tokens, understands over 140 languages, and offers improved math, reasoning, and chat capabilities, including structured outputs and function calling. Gemma 3 12B is the second largest in the family of Gemma 3 models after Gemma 3 27B

by google
33K context
$0/M input tokens
$0/M output tokens
ArliAI: QwQ 32B RpR v1 (free)

26,5M tokens
QwQ-32B-ArliAI-RpR-v1 is a 32B parameter model fine-tuned from Qwen/QwQ-32B using a curated creative writing and roleplay dataset originally developed for the RPMax series. It is designed to maintain coherence and reasoning across long multi-turn conversations by introducing explicit reasoning steps per dialogue turn, generated and refined using the base model itself.  The model was trained using RS-QLORA+ on 8K sequence lengths and supports up to 128K context windows (with practical performance around 32K). It is optimized for creative roleplay and dialogue generation, with an emphasis on minimizing cross-context repetition while preserving stylistic diversity.

by arliai
33K context
$0/M input tokens
$0/M output tokens
Dolphin3.0 R1 Mistral 24B (free)

23,3M tokens
Dolphin 3.0 R1 is the next generation of the Dolphin series of instruct-tuned models.  Designed to be the ultimate general purpose local model, enabling coding, math, agentic, function calling, and general use cases.  The R1 version has been trained for 3 epochs to reason using 800k reasoning traces from the Dolphin-R1 dataset.  Dolphin aims to be a general purpose reasoning instruct model, similar to the models behind ChatGPT, Claude, Gemini.  Part of the Dolphin 3.0 Collection Curated and trained by Eric Hartford, Ben Gitter, BlouseJury and Cognitive Computations

by cognitivecomputations
33K context
$0/M input tokens
$0/M output tokens
Google: Gemma 3 4B (free)

15M tokens
Gemma 3 introduces multimodality, supporting vision-language input and text outputs. It handles context windows up to 128k tokens, understands over 140 languages, and offers improved math, reasoning, and chat capabilities, including structured outputs and function calling.

by google
33K context
$0/M input tokens
$0/M output tokens
Google: Gemma 3n 4B (free)

13,5M tokens
Gemma 3n E4B-it is optimized for efficient execution on mobile and low-resource devices, such as phones, laptops, and tablets. It supports multimodal inputs—including text, visual data, and audio—enabling diverse tasks such as text generation, speech recognition, translation, and image analysis. Leveraging innovations like Per-Layer Embedding (PLE) caching and the MatFormer architecture, Gemma 3n dynamically manages memory usage and computational load by selectively activating model parameters, significantly reducing runtime resource requirements.  This model supports a wide linguistic range (trained in over 140 languages) and features a flexible 32K token context window. Gemma 3n can selectively load parameters, optimizing memory and computational efficiency based on the task or device capabilities, making it well-suited for privacy-focused, offline-capable applications and on-device AI solutions. Read more in the blog post

by google
8K context
$0/M input tokens
$0/M output tokens
NVIDIA: Llama 3.1 Nemotron Ultra 253B v1 (free)

Llama-3.1-Nemotron-Ultra-253B-v1 is a large language model (LLM) optimized for advanced reasoning, human-interactive chat, retrieval-augmented generation (RAG), and tool-calling tasks. Derived from Meta’s Llama-3.1-405B-Instruct, it has been significantly customized using Neural Architecture Search (NAS), resulting in enhanced efficiency, reduced memory usage, and improved inference latency. The model supports a context length of up to 128K tokens and can operate efficiently on an 8x NVIDIA H100 node.  Note: you must include detailed thinking on in the system prompt to enable reasoning. Please see Usage Recommendations for more.

by nvidia
131K context
$0/M input tokens
$0/M output tokens
Reka: Flash 3 (free)

Reka Flash 3 is a general-purpose, instruction-tuned large language model with 21 billion parameters, developed by Reka. It excels at general chat, coding tasks, instruction-following, and function calling. Featuring a 32K context length and optimized through reinforcement learning (RLOO), it provides competitive performance comparable to proprietary models within a smaller parameter footprint. Ideal for low-latency, local, or on-device deployments, Reka Flash 3 is compact, supports efficient quantization (down to 11GB at 4-bit precision), and employs explicit reasoning tags ("<reasoning>") to indicate its internal thought process.  Reka Flash 3 is primarily an English model with limited multilingual understanding capabilities. The model weights are released under the Apache 2.0 license.

by rekaai
33K context
$0/M input tokens
$0/M output tokens
DeepSeek: R1 Distill Qwen 14B (free)

DeepSeek R1 Distill Qwen 14B is a distilled large language model based on Qwen 2.5 14B, using outputs from DeepSeek R1. It outperforms OpenAI's o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.  Other benchmark results include:  - AIME 2024 pass@1: 69.7 - MATH-500 pass@1: 93.9 - CodeForces Rating: 1481  The model leverages fine-tuning from DeepSeek R1's outputs, enabling competitive performance comparable to larger frontier models.

by deepseek
64K context
$0/M input tokens
$0/M output tokens

'''