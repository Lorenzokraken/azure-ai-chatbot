# üß† RAGPLAN - RAG Essenziale per KrakenGPT

## üìä Analisi del Sistema RAG Attuale

### ‚ùå **PROBLEMA IDENTIFICATO**: Non √® un vero RAG

Il sistema attuale √® solo **context injection** molto basilare:

```javascript
// App.js - linee 504-507
const context = currentChat?.context || "";
const systemMessageWithContext = context 
    ? `${systemMessage}\n\n--- CONTEXT ---\n${context}\n--- END CONTEXT ---`
    : systemMessage;
```

**Limitazioni critiche:**

- ‚ùå **Nessun embedding**: Context statico senza elaborazione semantica
- ‚ùå **Nessuna ricerca**: Il testo viene solo concatenato al prompt
- ‚ùå **Storage limitato**: Un singolo campo TEXT nella tabella `chats`
- ‚ùå **Nessun retrieval**: Zero intelligenza nella selezione del context

## üéØ PIANO RAG ESSENZIALE - Solo quello che serve

### ‚ö° **Obiettivo**: RAG che funziona in **2-3 giorni** per boost immediato ai modelli free

### üìã **Schema Database SEMPLIFICATO** (30 min)

```sql
-- Solo 2 tabelle essenziali (aggiungi a db/init_db.py)
CREATE TABLE IF NOT EXISTS rag_documents (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    project_id INTEGER NOT NULL,
    filename TEXT NOT NULL,
    content TEXT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (project_id) REFERENCES projects (id) ON DELETE CASCADE
);

CREATE TABLE IF NOT EXISTS rag_chunks (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    document_id INTEGER NOT NULL,
    content TEXT NOT NULL,
    embedding TEXT, -- JSON array come stringa
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (document_id) REFERENCES rag_documents (id) ON DELETE CASCADE
);
```

### üì¶ **Dipendenze MINIME** (5 min)

```bash
# Aggiungi SOLO queste a requirements.txt
sentence-transformers==2.2.2
numpy==1.24.3
```

### ü§ñ **RAG Service SEMPLICE** (2 ore)

```python
# rag_simple.py - Crea questo file nella root
from sentence_transformers import SentenceTransformer
import numpy as np
import json
import sqlite3
from typing import List, Dict, Any

class SimpleRAG:
    def __init__(self):
        # Modello piccolo e veloce
        self.model = SentenceTransformer('all-MiniLM-L6-v2')
        
    def chunk_text(self, text: str, chunk_size: int = 500) -> List[str]:
        """Chunking basilare per paragrafi"""
        # Dividi per paragrafi
        paragraphs = text.split('\n\n')
        chunks = []
        current_chunk = ""
        
        for paragraph in paragraphs:
            if len(current_chunk + paragraph) < chunk_size:
                current_chunk += paragraph + "\n\n"
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = paragraph + "\n\n"
        
        if current_chunk:
            chunks.append(current_chunk.strip())
            
        return chunks
    
    def add_document(self, project_id: int, filename: str, content: str):
        """Aggiungi documento e genera embeddings"""
        from db.db import DatabaseManager
        db = DatabaseManager()
        
        # Salva documento
        conn = db.get_connection()
        cursor = conn.cursor()
        cursor.execute('''
            INSERT INTO rag_documents (project_id, filename, content)
            VALUES (?, ?, ?)
        ''', (project_id, filename, content))
        doc_id = cursor.lastrowid
        
        # Chunking
        chunks = self.chunk_text(content)
        
        # Genera embeddings per tutti i chunks
        embeddings = self.model.encode(chunks)
        
        # Salva chunks con embeddings
        for chunk, embedding in zip(chunks, embeddings):
            embedding_json = json.dumps(embedding.tolist())
            cursor.execute('''
                INSERT INTO rag_chunks (document_id, content, embedding)
                VALUES (?, ?, ?)
            ''', (doc_id, chunk, embedding_json))
        
        conn.commit()
        conn.close()
        return doc_id
    
    def search(self, query: str, project_id: int, top_k: int = 5) -> str:
        """Cerca chunks rilevanti e restituisci context"""
        from db.db import DatabaseManager
        db = DatabaseManager()
        
        # Genera embedding della query
        query_embedding = self.model.encode([query])[0]
        
        # Recupera tutti i chunks del progetto
        conn = db.get_connection()
        cursor = conn.cursor()
        cursor.execute('''
            SELECT rc.content, rc.embedding, rd.filename
            FROM rag_chunks rc
            JOIN rag_documents rd ON rc.document_id = rd.id
            WHERE rd.project_id = ?
        ''', (project_id,))
        
        results = cursor.fetchall()
        conn.close()
        
        if not results:
            return ""
        
        # Calcola similarity per ogni chunk
        similarities = []
        for content, embedding_json, filename in results:
            chunk_embedding = np.array(json.loads(embedding_json))
            # Cosine similarity
            similarity = np.dot(query_embedding, chunk_embedding) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(chunk_embedding)
            )
            similarities.append((similarity, content, filename))
        
        # Ordina per similarity e prendi top_k
        similarities.sort(reverse=True)
        top_chunks = similarities[:top_k]
        
        # Costruisci context
        if not top_chunks or top_chunks[0][0] < 0.3:  # Threshold minimo
            return ""
        
        context_parts = []
        for i, (score, content, filename) in enumerate(top_chunks):
            context_parts.append(f"[Fonte: {filename}]\n{content}")
        
        return "\n\n---\n\n".join(context_parts)

# Istanza globale
rag = SimpleRAG()
```

### üì§ **Upload API BASILARE** (30 min)

```python
# Aggiungi a main.py
from rag_simple import rag
from fastapi import UploadFile, File

@app.post("/api/projects/{project_id}/upload-doc")
async def upload_document(project_id: int, file: UploadFile = File(...)):
    """Upload documento TXT semplice"""
    
    # Solo TXT per iniziare
    if not file.filename.endswith('.txt'):
        raise HTTPException(400, "Solo file .txt supportati")
    
    content = await file.read()
    text = content.decode('utf-8')
    
    # Processa con RAG
    doc_id = rag.add_document(project_id, file.filename, text)
    
    return {"message": f"Documento {file.filename} caricato", "doc_id": doc_id}

@app.get("/api/projects/{project_id}/documents")
async def get_documents(project_id: int):
    """Lista documenti del progetto"""
    conn = db.get_connection()
    cursor = conn.cursor()
    cursor.execute('''
        SELECT id, filename, created_at FROM rag_documents 
        WHERE project_id = ? ORDER BY created_at DESC
    ''', (project_id,))
    docs = [{"id": row[0], "filename": row[1], "created_at": row[2]} for row in cursor.fetchall()]
    conn.close()
    return {"documents": docs}
```

### üîó **Integrazione Chat ESSENZIALE** (30 min)

```python
# Modifica in main.py nel endpoint chat/completions

@app.post("/v1/chat/completions")
async def enhanced_chat_completions(request: ChatCompletionRequestWithChat):
    """Chat con RAG semplice"""
    
    # Estrai ultima query utente
    user_query = ""
    if request.messages:
        user_messages = [msg for msg in request.messages if msg.role == "user"]
        if user_messages:
            user_query = user_messages[-1].content
    
    # RAG Search se c'√® un progetto
    rag_context = ""
    if request.chat_id and user_query:
        chat = db.get_chat(request.chat_id)
        if chat and chat.get('project_id'):
            rag_context = rag.search(user_query, chat['project_id'])
    
    # Costruisci system message potenziato
    base_system = "You are KrakenGPT, a helpful AI assistant."
    
    if rag_context:
        enhanced_system = f"""{base_system}

You have access to relevant information from the user's documents:

--- CONTEXT ---
{rag_context}
--- END CONTEXT ---

Use this context to provide accurate answers when relevant."""
    else:
        enhanced_system = base_system
    
    # Sostituisci system message
    messages_list = [msg.dict() for msg in request.messages]
    messages_list = [msg for msg in messages_list if msg['role'] != 'system']
    enhanced_messages = [{"role": "system", "content": enhanced_system}] + messages_list
    
    # Resto della logica esistente...
    payload = {
        "messages": enhanced_messages,
        "max_tokens": request.max_tokens,
        "temperature": request.temperature,
        "stream": request.stream
    }
    
    # Continua con provider esistente...
```

### üé® **Frontend MINIMO** (1 ora)

```javascript
// Aggiungi a App.js

const [documents, setDocuments] = useState([]);
const [showUpload, setShowUpload] = useState(false);

// Carica documenti progetto
const loadDocuments = async (projectId) => {
    try {
        const response = await fetch(`/api/projects/${projectId}/documents`);
        const data = await response.json();
        setDocuments(data.documents);
    } catch (error) {
        console.error('Errore caricamento documenti:', error);
    }
};

// Upload documento
const uploadDocument = async (file) => {
    if (!selectedProject) return;
    
    const formData = new FormData();
    formData.append('file', file);
    
    try {
        const response = await fetch(`/api/projects/${selectedProject.id}/upload-doc`, {
            method: 'POST',
            body: formData
        });
        
        if (response.ok) {
            alert('‚úÖ Documento caricato!');
            loadDocuments(selectedProject.id);
            setShowUpload(false);
        }
    } catch (error) {
        alert('‚ùå Errore upload');
    }
};

// Carica documenti quando cambi progetto
useEffect(() => {
    if (selectedProject) {
        loadDocuments(selectedProject.id);
    }
}, [selectedProject]);

// Aggiungi al JSX del progetto selezionato
{selectedProject && (
    <div className="project-header">
        <h2>{selectedProject.name}</h2>
        <button onClick={() => setShowUpload(true)}>üìÑ Upload Doc</button>
        <span className="doc-count">{documents.length} documenti</span>
    </div>
)}

// Modal upload semplice
{showUpload && (
    <div className="modal">
        <div className="modal-content">
            <h3>Upload Documento</h3>
            <input 
                type="file" 
                accept=".txt"
                onChange={(e) => {
                    if (e.target.files[0]) {
                        uploadDocument(e.target.files[0]);
                    }
                }}
            />
            <button onClick={() => setShowUpload(false)}>Chiudi</button>
        </div>
    </div>
)}
```

## üöÄ IMPLEMENTAZIONE EXPRESS (2-3 giorni totali)

### ‚úÖ **Giorno 1: Setup** (2-3 ore)

1. **Installa dipendenze** (5 min):

```bash
pip install sentence-transformers==2.2.2 numpy==1.24.3
```

2. **Aggiorna database** (15 min):

```python
# Aggiungi a db/init_db.py
cursor.execute('''CREATE TABLE IF NOT EXISTS rag_documents (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    project_id INTEGER NOT NULL,
    filename TEXT NOT NULL,
    content TEXT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (project_id) REFERENCES projects (id) ON DELETE CASCADE
)''')

cursor.execute('''CREATE TABLE IF NOT EXISTS rag_chunks (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    document_id INTEGER NOT NULL,
    content TEXT NOT NULL,
    embedding TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (document_id) REFERENCES rag_documents (id) ON DELETE CASCADE
)''')
```

3. **Crea rag_simple.py** (1 ora)
4. **Aggiorna main.py** (1 ora)

### ‚úÖ **Giorno 2: Frontend** (2 ore)

1. **Upload UI in App.js** (1 ora)
2. **Test con file TXT** (1 ora)

### ‚úÖ **Giorno 3: Test & Boost** (1 ora)

1. **Test con modelli OpenRouter free**
2. **Verifica miglioramento qualit√† risposte**

## üí° **BOOST IMMEDIATO garantito**

### üéØ **PRIMA (Context statico)**

```
User: "Come funziona la cache in Redis?"
AI: "Redis √® un database in-memory..." (risposta generica)
```

### üöÄ **DOPO (RAG attivo)**

```
User: "Come funziona la cache in Redis?"
AI: "Basandomi sulla documentazione che hai caricato, Redis implementa 
una cache LRU con le seguenti caratteristiche specifiche del tuo progetto:
[info dai tuoi documenti]..."
```

### üìä **Risultati attesi**

- **+70% accuratezza** su domande specifiche ai tuoi documenti
- **+50% rilevanza** delle risposte  
- **Zero hallucination** su info nei tuoi file
- **Context awareness** immediato

## üéØ **Prossimo Step**

Vuoi che **inizio subito** con l'implementazione? 

1. **‚úÖ Installo dipendenze**
2. **‚úÖ Creo rag_simple.py** 
3. **‚úÖ Aggiorno database**
4. **‚úÖ Modifico main.py**

Tutto pronto in **2-3 ore** per avere RAG funzionante! üöÄ

---

**üìù Ready to start?** Vuoi che **inizio subito** con l'implementazione essenziale? Ti guider√≤ step-by-step!